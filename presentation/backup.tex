\begin{frame}
  \frametitle{Artificial Neural Networks}
  \begin{itemize}
    \item Class of machine learning algorithms
      \begin{itemize}
        \item Loosely inspired by biological nervous systems
      \end{itemize}
    \item Collection of artificial neurons that are connected with
      each other
      \begin{itemize}
        \item Enables them to exchange signals along their connections
        \item Can be represented by a directed graph
      \end{itemize}
    \item Usually arranged in layers
      \begin{itemize}
        \item \textit{Input Layer} collects input signals and passes
          them on
        \item \textit{Hidden Layers} apply transformations to incoming
          signals and pass the outcomes further into the network
        \item \textit{Output Layer} applies a final transformation
          representing the networks' result
      \end{itemize}
    \item Goal: Convert input into meaningful output by applying
      multiple transformations
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Components of the neural model}
  \begin{itemize}
   \item A set of weighted inputs
     \begin{itemize}
       \item Each input originating from neuron \(j\) and traveling
         into neuron \(k\) is first multiplied by a weight \(w_{kj}\)
     \end{itemize}
   \item A summation unit
     \begin{itemize}
       \item All the weighted inputs are summed and a constant value,
         the \textit{bias}, is added to yield the result \(z_k\)
     \end{itemize}
   \item An activation function
     \begin{itemize}
       \item Applies a non-linear transformation \(\phi(\cdot)\) to
         the output of the summation unit
       \item This result, called \(y_k\), is propagated further into
         the network alongside the connections
     \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The Role of the Bias Value}
  \begin{itemize}
    \item The bias is added as a constant to the sum of the weighted
      inputs in the summation unit
    \item Acts like a threshold that has to be overcome
      \begin{itemize}
        \item Negative bias: Positive weighted inputs needed for the
          neuron to become active
        \item Positive bias: Negative weighted inputs needed to stop
          the neuron from being active
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The Role of the Bias Value}
  \begin{figure}
    \resizebox{.6\textwidth}{!}{\input{../figures/bias}}
    \caption{The sigmoid activation function plotted for different bias values.}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Evaluation Metrics}
  \begin{itemize}
    \item \textbf{Accuracy:}
      \begin{itemize}
        \item Percentage of testing examples that were classified correctly
      \end{itemize}
      \begin{equation}
        Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
      \end{equation}
    \item \textbf{Precision:}
      \begin{itemize}
        \item Percentage of correctly classified examples among all
          examples classified as positive
      \end{itemize}
      \begin{equation}
        Precision = \frac{TP}{TP + FP}
      \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Evaluation Metrics}
  \begin{itemize}
    \item \textbf{Recall:}
      \begin{itemize}
        \item What percentage of positive examples was classified
          correctly?
      \end{itemize}
      \begin{equation}
        Recall = \frac{TP}{TP + FN}
      \end{equation}
    \item \textbf{F1 Score:}
      \begin{itemize}
        \item Harmonic mean of precision and recall
      \end{itemize}
      \begin{equation}
        \text{F1 Score} = \frac{2*Precision*Recall}{Precision+Recall}
      \end{equation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dead Wire Classifier}
  \begin{itemize}
  \item Accuracy: 97.61\%
  \item Precision: 99.96\%
  \item Recall: 95.65\%
  \item F-Measure: 97.76\%
\end{itemize}
\begin{table}[h]
  \centering
  \renewcommand\theadfont{\bfseries}
  \begin{tabular}{|c|c|c|}
    \hline
    & \thead{Dead Wire\\(Predicted)} & \thead{No Dead Wire\\(Predicted)} \\
    \hline
    \thead{Dead Wire\\(Actual)} & 5212 & 237\\
    \hline
    \thead{No Dead Wire\\(Actual)} & 2 & 4549\\
    \hline
  \end{tabular}
  \caption{Confusion matrix of the dead wire classifier.}
\end{table}
\end{frame}

\begin{frame}
  \frametitle{Dead Pin Classifier}
  \begin{itemize}
  \item Accuracy: 99.95\%
  \item Precision: 99.92\%
  \item Recall: 99.98\%
  \item F-Measure: 99.95\%
\end{itemize}
\begin{table}[h]
  \centering
  \renewcommand\theadfont{\bfseries}
  \begin{tabular}{|c|c|c|}
    \hline
    & \thead{Dead Pin\\(Predicted)} & \thead{No Dead Pin\\(Predicted)} \\
    \hline
    \thead{Dead Pin\\(Actual)} & 4739 & 1\\
    \hline
    \thead{No Dead Pin\\(Actual)} & 4 & 5256\\
    \hline
  \end{tabular}
  \caption{Confusion matrix of the dead pin classifier.}
\end{table}
\end{frame}

\begin{frame}
  \frametitle{Dead Connector Classifier}
  \begin{itemize}
  \item Accuracy: 98.77\%
  \item Precision: 99.23\%
  \item Recall: 95.69\%
  \item F-Measure: 97.43\%
\end{itemize}
\begin{table}[h]
  \centering
  \renewcommand\theadfont{\bfseries}
  \begin{tabular}{|c|c|c|}
    \hline
    & \thead{Dead Connector\\(Predicted)} & \thead{No Dead Connector\\(Predicted)} \\
    \hline
    \thead{Dead Connector\\(Actual)} & 2334 & 105\\
    \hline
    \thead{No Dead Connector\\(Actual)} & 18 & 7543\\
    \hline
  \end{tabular}
  \caption{Confusion matrix of the dead connector classifier.}
\end{table}
\end{frame}

\begin{frame}
  \frametitle{Dead Fuse Classifier}
  \begin{itemize}
  \item Accuracy: 98.95\%
  \item Precision: 97.32\%
  \item Recall: 98.20\%
  \item F-Measure: 97.76\%
\end{itemize}
\begin{table}[h]
  \centering
  \renewcommand\theadfont{\bfseries}
  \begin{tabular}{|c|c|c|}
    \hline
    & \thead{Dead Fuse\\(Predicted)} & \thead{No Dead Fuse\\(Predicted)} \\
    \hline
    \thead{Dead Fuse\\(Actual)} & 2288 & 42\\
    \hline
    \thead{No Dead Fuse\\(Actual)} & 63 & 7607\\
    \hline
  \end{tabular}
  \caption{Confusion matrix of the dead fuse classifier.}
\end{table}
\end{frame}

\begin{frame}
  \frametitle{Dead Channel Classifier}
  \begin{itemize}
  \item Accuracy: 99.11\%
  \item Precision: 98.84\%
  \item Recall: 98.64\%
  \item F-Measure: 98.74\%
\end{itemize}
\begin{table}[h]
  \centering
  \renewcommand\theadfont{\bfseries}
  \begin{tabular}{|c|c|c|}
    \hline
    & \thead{Dead Channel\\(Predicted)} & \thead{No Dead Channel\\(Predicted)} \\
    \hline
    \thead{Dead Channel\\(Actual)} & 3493 & 48\\
    \hline
    \thead{No Dead Channel\\(Actual)} & 41 & 6418\\
    \hline
  \end{tabular}
  \caption{Confusion matrix of the dead channel classifier.}
\end{table}
\end{frame}

\begin{frame}
  \frametitle{Hot Wire Classifier}
  \begin{itemize}
  \item Accuracy: 100.00\%
  \item Precision: 100.00\%
  \item Recall: 100.00\%
  \item F-Measure: 100.00\%
\end{itemize}
\begin{table}[h]
  \centering
  \renewcommand\theadfont{\bfseries}
  \begin{tabular}{|c|c|c|}
    \hline
    & \thead{Hot Wire\\(Predicted)} & \thead{No Hot Wire\\(Predicted)} \\
    \hline
    \thead{Hot Wire\\(Actual)} & 5532 & 0\\
    \hline
    \thead{No Hot Wire\\(Actual)} & 0 & 4468\\
    \hline
  \end{tabular}
  \caption{Confusion matrix of the hot wire classifier.}
\end{table}
\end{frame}
