\chapter{Deep Learning Fundamentals}

\section{Artificial Neural Networks}

Artificial neural networks (ANNs) are a class of machine learning algorithms
that are loosely inspired by the structure of biological nervous
systems.
To be precise, each ANN consists of a collection of artificial neurons
that are connected with each other. The neurons are able to exchange
information along their connections.
A common way to arrange artificial neurons within a network is to organize
them in layers as depicted in \fref{fig:basic-network}.
\begin{figure}[h]
  \centering
  \resizebox{0.75\textwidth}{!}{\input{../figures/basic_network}}
  \caption{The structure of a simple ANN. The nodes represent the
    neurons, the edges represent their connections, also indicating
    the flow of information.}
  \label{fig:basic-network}
\end{figure}

When an artificial neuron receives a signal on some of its
incoming connections, it may elect to become active.\footnote{The
  details of this
  process are further illustrated in \fref{sec:artificial-neurons}.}
In this state it also influences all neurons it has an outgoing
connection to by passing a signal along their
channel. Those other neurons in turn may also elect to become
active - this way a signal can propagate through the network along
the connecting edges.

Usually, each ANN consists of at least one layer of neurons that is
responsible for receiving signals from the environment - we call this
an \textit{input layer} (see \fref{fig:basic-network}). When these neurons
receive a signal from the environment, they propagate it to their
connected neighbors in the next layer. This process repeats until the
\textit{output layer} is reached. The neurons in this layer represent the
output of the whole network. Each layer in between is called a \textit{hidden
layer} because there is no direct communication between the neurons in
this layer and the environment.

The goal behind this procedure usually is to convert an input signal
into a meaningful output by feeding it through the network. If the
network is able to detect relevant features or patterns in the input
signal, it can be used to perform tasks such as classification or
regression (i.e. approximate discrete or continuous functions).
In order for this to be possible, some kind of learning has to take
place which enables the network to capture the essence of the data it
is confronted with. We will take a further look at these aspects as
well as the mathematical model of a neural network in the following
sections.

\subsection{Modeling Artificial Neurons}
\label{sec:artificial-neurons}
To fully understand how each neuron processes the signals it receives,
it is necessary to develop a mathematical model that describes all the
operations taking place. The following descriptions are partially
based on the explanations that are provided in \cite{Haykin}.\footnote{See
  chapter I.3: \textit{Models of a Neuron} for more details.}
As shown in \fref{fig:single-neuron}, each artificial neuron
basically consists of three components:
\begin{enumerate}
  \item \textbf{A set of weighted inputs:} Each connection that is
    leading into the neuron has a weight \(w_{kj}\) associated with it
    where \(k\) denotes the neuron in question and \(j\) denotes the
    index of the neuron that delivers its input to the current neuron
    \(k\).\footnote{There might arise the question why the indexing of
    a weight from neuron \(j\) to neuron \(k\) is \(w_{kj}\) and
    \textit{not} \(w_{jk}\). This is the case because the weights are
    usually stored in matrices where each row corresponds to a
    neuron \(k\) and each column corresponds to an input \(j\) which
    allows for much faster computations by heavily utilizing
    matrix-multiplication.} The signal that
    passes the connection is multiplied by the
    related weight of that connection before arriving at the next
    component.
  \item \textbf{A summation unit:} This component adds up all the
    weighted signals that arrive at the neuron as well as a bias value
    \(b_k\) that is independent of the inputs.
  \item \textbf{An activation function:} The activation function
    \(\phi(\cdot)\) takes the output of the summation unit and applies
    a transformation to it that is usually non-linear. The value of
    the activation function is the output of the neuron which will
    travel further through the network alongside the corresponding
    connections. In \fref{sec:activation-functions}, a more
    detailed explanation of activation functions as well as some
    commonly used examples will be provided.
\end{enumerate}
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{../figures/single_neuron}
  \caption{The components of a single artificial neuron. This neuron
    \(k\) receives three input signals that are first multiplied by
    the associated weights, summed up including a bias and then fed
    into an activation function that will determine the ouput signal.}
  \label{fig:single-neuron}
\end{figure}
As a result, the output of the summation unit of a particular
neuron \(k\) with \(n\) input signals \(x_j\) can be described by the
following equation:
\begin{equation}
  z_k = \sum_{j=1}^{n}{x_j \cdot w_{kj}} + b_k
\end{equation}
where \(b_k\) denotes the bias term of neuron \(k\) and \(z_k\) describes the
result of the summation unit.

As a consequence, the output signal \(y_k\) of neuron \(k\) can be computed by
applying the activation function \(\phi(\cdot)\) to the output of the
summation unit which can be described by the following expression:
\begin{equation}
  y_k = \phi(z_k)
\end{equation}

\subsection{Activation Functions}
\label{sec:activation-functions}


\section{The Multilayer Perceptron}

\section{Deep Networks}
